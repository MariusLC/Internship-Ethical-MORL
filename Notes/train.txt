notes sur les fichiers train :



sur le readME : 

There are four training scripts for

    manually training a PPO agent on custom rewards (ppo_train.py),
    training AIRL on a single expert dataset (airl_train.py),
    active MORL with custom/automatic preferences (moral_train.py) and
    training DRLHP with custom/automatic preferences (drlhp_train.py).

When using automatic preferences, a desired ratio can be passed as an argument. For example,

python moral_train.py --ratio a b c

will run MORAL using a (real-valued) ratio of a:b:c among the three explicit objectives in Delivery.



Le fichier moral_train.py lance manuellement l'experience delivery avec les expert définis en dur, car ils import des fichiers précis des agents ppo précédement entrainés. 
-> on pourrait changer le système pour que ça lance une itération de l'algorithme complet selon un fichier de config (avec dedans le nom des fichiers de politique des experts ou leurs préférences si l'on veut que ça relance l'étape d'AIRL).

 
 
 
 
 !!!!!!!!!!!!!!!!!!!
 PROBLEMES D EXECUTION
 j'arrive pas à lancer les fichiers qui sont dans le dossier "moral" (tous les fichiers de train) car j'ai une erreur à la ligne "from envs.gym_wrapper import ", qui dit : "no module named envs". Elle vient du fait que j'execute les fichiers depuis le dossier moral et pas depuis moral_rl le dossier parent et j'ai donc pas accès au dossier envs. Sauf que si j'execute depuis le dossier parent en temps que module : python3 -m moral.moral_train (au lieu de python3 moral/moral_train.py), j'ai une erreur avec les import qui concernent les autres fichiers du dossier moral ("from ppo import PPO" par exemple).
 
 -> solution : je vais modifier les fichiers pour qu'ils import tous moral.ppo au lieu de juste import ppo. Puis les executer en temps que module depuis le fichier parent.
 EXEMPLE : pour executer ppo_train.py, je me place dans moral_rl et je lance "python3 -m moral.ppo_train".
 
 !!!!!!!!!!!!!!!!!!!!!!!!!!
 
 
 
 
 
 README : 


ppo_train.py
apprendre des agents ppo avec des poids fixés manuelement : python3 -m moral.ppo_train --lambd 0 0 1 1 		| avec 0 0 1 1 les poids correspondant respectivement aux livraisons, humains sauvés, tuiles nettoyées et vases cassés.




IDEE GLOBALE ALGO


poids lambda d'un expert -> ppo_train.py -> expert entrainé, agent ppo ->  utils.generate_demos_main.py  -> démonstrations de l'expert -> airl.py -> poids de l'expert estimés + poids fixé de l'expert de preference learning -> MORL -> poids estimés de l'expert de preference learning -> comportements d'un expert ppo qui est l'aggreggation des fonctions de récompense des experts avec les poids de l'expert de preference learning
