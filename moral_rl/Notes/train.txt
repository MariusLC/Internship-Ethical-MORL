notes sur les fichiers train :


!!!!! QUESTION SUR LE MODELE
Dans le fichier moral_train.py, ona accès directement aux politiques des agents ppo (les experts cachés de l'étpae d'AIRL) et non à leur démonstrations. Ils utilisent notamment leurs politiques pour calculer "l'utopie" (ligne 100). Il faut que je me renseigne sur ce que ce calcul fait et à quoi il correspond dans le modèle... On ne devrait pas avoir accès directement aux agents PPO !!
!!!!!!!!!!!!


Le fichier moral_train.py lance manuellement l'experience delivery avec les expert définis en dur, car ils import des fichiers précis des agents ppo précédement entrainés. 
-> on pourrait changer le système pour que ça lance une itération de l'algorithme complet selon un fichier de config (avec dedans le nom des fichiers de politique des experts ou leurs préférences si l'on veut que ça relance l'étape d'AIRL).

 
 
 
 
 !!!!!!!!!!!!!!!!!!!
 PROBLEMES D EXECUTION
 j'arrive pas à lancer les fichiers qui sont dans le dossier "moral" (tous les fichiers de train) car j'ai une erreur à la ligne "from envs.gym_wrapper import ", qui dit : "no module named envs". Elle vient du fait que j'execute les fichiers depuis le dossier moral et pas depuis moral_rl le dossier parent et j'ai donc pas accès au dossier envs. Sauf que si j'execute depuis le dossier parent en temps que module : python3 -m moral.moral_train (au lieu de python3 moral/moral_train.py), j'ai une erreur avec les import qui concernent les autres fichiers du dossier moral ("from ppo import PPO" par exemple).
 
 -> solution : je vais modifier les fichiers pour qu'ils import tous moral.ppo au lieu de juste import ppo. Puis les executer en temps que module depuis le fichier parent.
 EXEMPLE : pour executer ppo_train.py, je me place dans moral_rl et je lance "python3 -m moral.ppo_train".
 
 !!!!!!!!!!!!!!!!!!!!!!!!!!
 
 



IDEE GLOBALE ALGO


poids lambda d'un expert -> ppo_train.py -> expert entrainé, agent ppo ->  utils.generate_demos_main.py  -> démonstrations de l'expert -> airl.py -> poids de l'expert estimés + poids fixé de l'expert de preference learning -> MORL -> poids estimés de l'expert de preference learning -> comportements d'un expert ppo qui est l'aggreggation des fonctions de récompense des experts avec les poids de l'expert de preference learning
