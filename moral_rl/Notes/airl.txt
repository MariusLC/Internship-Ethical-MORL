DiscriminatorMLP = discriminator avec MLP = Multilayer perceptron, un type de NN.


Discriminator vs DiscriminatorMLP = DiscriminatorMLP a uniquement des couches linéaires alors que Discriminator a des couches de convolutions

g = réseau qui apprend la Q function, soit la reward function. C'est à dire la récompense prédite à prendre une action dans un tel état, puis à suivre la politique.
h = réseau qui apprend la V function, soit la value function. C'est à dire la récompense prédite à prendre à être dans un état, puis suivre la politique.


fonctions :

discriminate : équivalent à l'équation D_theta(s,a) dans l'article.

estimate_utopia : fait un rollout dans l'environnement de n steps, accumule les rewards prédits par la politique au moment de faire le choix de l'action (et non les rewards EFFECTIFS) et fait la moyenne de ces rewards prédits. La fonction appelle la fonction forward pour accumuler les rewards prédits (donc plutôt les advantages en réalité). La moyenne des advantages est stockées dans la variable utopia_point.

training_sampler : On collecte l'équivalent de batch_size data par rapport à l'agent ppo. La moitié viennent de trajectoires expertes, l'autres de trajectoires générées par le générateur qui approxime l'expert (GAN). A chaque itération, on choisit n'importe quelle état de n'importe quelle trajectoire et on stocke dans un buffer la probabilité qu'à notre agent ppo courant de prendre l'action choisie pendant la trajectoire. On retourne ce batch de probabilité à la fin de la fonction.

update_discriminator : On récupère la buffer de probas de training_sampler, on calcule les advantages des actions choisies au cours des trajectories (avec la fonction discriminate). On créer un tensor 2D avec les log proba des actions choisies récupérés avec un forward de PPO (notre agent générateur donc) et les advantages de notre discriminant. On calcule l'erreur du discriminant : si la log proba des actions de ppo est supérieur à l'advantage du discriminant on considère qu'il pense que c'est une trajectorie experte, sinon que c'est une trjectoire géénrée. On possède les vrais labels des trajectoires grâce à training_sampler. On calcule alors la loss à l'aide d'une  backprop avec nn.CrossEntropyLoss() comme fonction de loss auquel on donne notre tensor de prediction et nos labels, et on effectue un step de descente de gradient.


-> reviser la cross entropy pour comprendre comment la loss est compute à partir des labels et des log porba et advantages du dicriminant et du générateur

